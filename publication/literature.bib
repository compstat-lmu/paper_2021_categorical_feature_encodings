@article{young_1976,
  title={Regression with qualitative and quantitative variables: An alternating least squares method with optimal scaling features},
  author={Young, Forrest W and De Leeuw, Jan and Takane, Yoshio},
  journal={Psychometrika},
  volume={41},
  number={4},
  pages={505--529},
  year={1976},
  publisher={Springer},
  doi = {https://doi.org/10.1007/BF02296972}
}

@article{rodriguez_2018,
title = {Beyond one-hot encoding: Lower dimensional target embedding},
journal = {Image and Vision Computing},
volume = {75},
pages = {21-31},
year = {2018},
issn = {0262-8856},
doi = {https://doi.org/10.1016/j.imavis.2018.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S0262885618300623},
author = {Pau Rodríguez and Miguel A. Bautista and Jordi Gonzàlez and Sergio Escalera},
keywords = {Error correcting output codes, Output embeddings, Deep learning, Computer vision},
abstract = {Target encoding plays a central role when learning Convolutional Neural Networks. In this realm, one-hot encoding is the most prevalent strategy due to its simplicity. However, this so widespread encoding schema assumes a flat label space, thus ignoring rich relationships existing among labels that can be exploited during training. In large-scale datasets, data does not span the full label space, but instead lies in a low-dimensional output manifold. Following this observation, we embed the targets into a low-dimensional space, drastically improving convergence speed while preserving accuracy. Our contribution is two fold: (i) We show that random projections of the label space are a valid tool to find such lower dimensional embeddings, boosting dramatically convergence rates at zero computational cost; and (ii) we propose a normalized eigenrepresentation of the class manifold that encodes the targets with minimal information loss, improving the accuracy of random projections encoding while enjoying the same convergence rates. Experiments on CIFAR-100, CUB200-2011, Imagenet, and MIT Places demonstrate that the proposed approach drastically improves convergence speed while reaching very competitive accuracy rates.}
}

@article{potdar_2017,
	author = {Kedar Potdar and Taher S. Pardawala and Chinmay D. Pai},
	title = {A Comparative Study of Categorical Variable Encoding Techniques for Neural Network Classifiers},
	journal = {International Journal of Computer Applications},
	issue_date = {October 2017},
	volume = {175},
	number = {4},
	month = {Oct},
	year = {2017},
	issn = {0975-8887},
	pages = {7-9},
	numpages = {3},
	url = {http://www.ijcaonline.org/archives/volume175/number4/28474-2017915495},
	doi = {10.5120/ijca2017915495},
	publisher = {Foundation of Computer Science (FCS), NY, USA},
	address = {New York, USA}
}

@InProceedings{seca_2021,
author="Seca, Diogo
and Mendes-Moreira, Jo{\~a}o",
editor="Rocha, {\'A}lvaro
and Adeli, Hojjat
and Dzemyda, Gintautas
and Moreira, Fernando
and Ramalho Correia, Ana Maria",
title="Benchmark of Encoders of Nominal Features for Regression",
booktitle="Trends and Applications in Information Systems and Technologies",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="146--155",
abstract="Mixed-type data is common in the real world. However, supervised learning algorithms such as support vector machines or neural networks can only process numerical features. One may choose to drop qualitative features, at the expense of possible loss of information. A better alternative is to encode them as new numerical features. Under the constraints of time, budget, and computational resources, we were motivated to search for a general-purpose encoder but found the existing benchmarks to be limited. We review these limitations and present an alternative. Our benchmark tests 16 encoding methods, on 15 regression datasets, using 7 distinct predictive models. The top general-purpose encoders were found to be Catboost, LeaveOneOut, and Target.",
isbn="978-3-030-72657-7"
}


@article{deleeuw_1976,
  title={Additive structure in qualitative data: An alternating least squares method with optimal scaling features},
  author={De Leeuw, Jan and Young, Forrest W and Takane, Yoshio},
  journal={Psychometrika},
  volume={41},
  number={4},
  pages={471--503},
  year={1976},
  publisher={Springer}
}

@article{mair_2010,
  title={A general framework for multivariate analysis with optimal scaling: The R package aspect},
  author={Mair, Patrick and de Leeuw, Jan},
  journal={Journal of Statistical Software},
  volume={32},
  number={1},
  pages={1--23},
  year={2010},
  doi={10.18637/jss.v032.i09}
}

@misc{niessl_2021,
      title={Over-optimism in benchmark studies and the multiplicity of design and analysis options when interpreting their results}, 
      author={Christina Nießl and Moritz Herrmann and Chiara Wiedemann and Giuseppe Casalicchio and Anne-Laure Boulesteix},
      year={2021},
      eprint={2106.02447},
      archivePrefix={arXiv},
      primaryClass={stat.ME}
}

@article{boulesteix_2017,
  title={On the necessity and design of studies comparing statistical methods.},
  author={Boulesteix, Anne-Laure and Binder, Harald and Abrahamowicz, Michal and Sauerbrei, Willi and others},
  journal={Biometrical journal. Biometrische Zeitschrift},
  volume={60},
  number={1},
  pages={216--218},
  year={2017},
  doi={10.1002/bimj.201700129}
}

@article{boulesteix_2020,
  title={A replication crisis in methodological research?},
  author={Boulesteix, Anne-Laure and Hoffmann, Sabine and Charlton, Alethea and Seibold, Heidi},
  journal={Significance},
  volume={17},
  number={5},
  pages={18--21},
  year={2020},
  publisher={Wiley Online Library},
  doi={10.1111/1740-9713.0144}
}

@article{chambers_1992,
  title={Statistical models. Chapter 2 of Statistical Models in S},
  author={Chambers, JM and Hastie, TJ},
  journal={Wadsworth \& Brooks/Cole},
  year={1992}
}

@Manual{meyer_2018,
  title = {relations: Data Structures and Algorithms for Relations},
  author = {David Meyer and Kurt Hornik},
  year = {2018},
  note = {R package version 0.6-8},
  url = {https://CRAN.R-project.org/package=relations},
}

@article{nadeau_2003,
  author    = {Claude Nadeau and
               Yoshua Bengio},
  title     = {Inference for the Generalization Error},
  journal   = {Machine Learning},
  volume    = {52},
  number    = {3},
  pages     = {239--281},
  year      = {2003},
  url       = {https://doi.org/10.1023/A:1024068626366},
  doi       = {10.1023/A:1024068626366}
}

@Manual{r_2021,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2021},
    url = {https://www.R-project.org/},
  }

@misc{dua_2017,
author = "Dua, Dheeru and Graff, Casey",
year = "2017",
title = "{UCI} Machine Learning Repository",
url = "http://archive.ics.uci.edu/ml",
institution = "University of California, Irvine, School of Information and Computer Sciences" }

@article{eugster_2014,
  title={(Psycho-) analysis of benchmark experiments: A formal framework for investigating the relationship between data sets and learning algorithms},
  author={Eugster, Manuel JA and Leisch, Friedrich and Strobl, Carolin},
  journal={Computational Statistics \& Data Analysis},
  volume={71},
  pages={986--1000},
  year={2014},
  publisher={Elsevier},
  doi={10.1016/j.csda.2013.08.007}
}

@article{delgado,
  author  = {Manuel Fern{{\'a}}ndez-Delgado and Eva Cernadas and Sen{{\'e}}n Barro and Dinani Amorim},
  title   = {Do we Need Hundreds of Classifiers to Solve Real World Classification Problems?},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  number  = {90},
  pages   = {3133-3181},
  url     = {http://jmlr.org/papers/v15/delgado14a.html}
}

@ARTICLE{bommert,
title = {Benchmark for filter methods for feature selection in high-dimensional classification data},
author = {Bommert, Andrea and Sun, Xudong and Bischl, Bernd and Rahnenführer, Jörg and Lang, Michel},
year = {2020},
journal = {Computational Statistics & Data Analysis},
volume = {143},
number = {C},
keywords = {Feature selection; Filter methods; High-dimensional data; Benchmark;},
url = {https://EconPapers.repec.org/RePEc:eee:csdana:v:143:y:2020:i:c:s016794731930194x},
doi = {10.1016/j.csda.2019.106839}
}

@article{hancock,
  title={Survey on categorical data for neural networks},
  author={Hancock, John T and Khoshgoftaar, Taghi M},
  journal={Journal of Big Data},
  volume={7},
  pages={1--41},
  year={2020},
  publisher={Springer},
  doi = {10.1186/s40537-020-00305-w}
}

@article{hothorn_2005,
author = {Torsten Hothorn and Friedrich Leisch and Achim Zeileis and Kurt Hornik},
title = {The Design and Analysis of Benchmark Experiments},
journal = {Journal of Computational and Graphical Statistics},
volume = {14},
number = {3},
pages = {675-699},
year  = {2005},
publisher = {Taylor & Francis},
doi = {10.1198/106186005X59630},

URL = {
        https://doi.org/10.1198/106186005X59630

},
eprint = {
        https://doi.org/10.1198/106186005X59630

}

}


@article{shah_2014,
    author = {Shah, Anoop D. and Bartlett, Jonathan W. and Carpenter, James and Nicholas, Owen and Hemingway, Harry},
    title = "{Comparison of Random Forest and Parametric Imputation Models for Imputing Missing Data Using MICE: A CALIBER Study}",
    journal = {American Journal of Epidemiology},
    volume = {179},
    number = {6},
    pages = {764-774},
    year = {2014},
    month = {01},
    abstract = "{Multivariate imputation by chained equations (MICE) is commonly used for imputing missing data in epidemiologic research. The “true” imputation model may contain nonlinearities which are not included in default imputation models. Random forest imputation is a machine learning technique which can accommodate nonlinearities and interactions and does not require a particular regression model to be specified. We compared parametric MICE with a random forest-based MICE algorithm in 2 simulation studies. The first study used 1,000 random samples of 2,000 persons drawn from the 10,128 stable angina patients in the CALIBER database (Cardiovascular Disease Research using Linked Bespoke Studies and Electronic Records; 2001–2010) with complete data on all covariates. Variables were artificially made “missing at random,” and the bias and efficiency of parameter estimates obtained using different imputation methods were compared. Both MICE methods produced unbiased estimates of (log) hazard ratios, but random forest was more efficient and produced narrower confidence intervals. The second study used simulated data in which the partially observed variable depended on the fully observed variables in a nonlinear way. Parameter estimates were less biased using random forest MICE, and confidence interval coverage was better. This suggests that random forest imputation may be useful for imputing complex epidemiologic data sets in which some patients have missing data.}",
    issn = {0002-9262},
    doi = {10.1093/aje/kwt312},
    url = {https://dx.doi.org/10.1093/aje/kwt312},
    eprint = {http://oup.prod.sis.lan/aje/article-pdf/179/6/764/17341607/kwt312.pdf},
}

@article{tutz_2016,
author = {Gerhard Tutz and Jan Gertheiss},
title ={Rejoinder: Regularized regression for categorical data},
journal = {Statistical Modelling},
volume = {16},
number = {3},
pages = {249-260},
year = {2016},
doi = {10.1177/1471082X16652780},

URL = {
        https://doi.org/10.1177/1471082X16652780

},
eprint = {
        https://doi.org/10.1177/1471082X16652780

}

}


@article{chiquet_2016,
author = {Julien Chiquet and Yves Grandvalet and Guillem Rigaill},
title ={On coding effects in regularized categorical regression},
journal = {Statistical Modelling},
volume = {16},
number = {3},
pages = {228-237},
year = {2016},
doi = {10.1177/1471082X16644998},

URL = {
        https://doi.org/10.1177/1471082X16644998

},
eprint = {
        https://doi.org/10.1177/1471082X16644998

}
,
    abstract = { This discussion is a continuation of Tutz and Gertheiss (2016)’s paper, where we focus on the importance of the coding of effects in regularized categorical and ordinal regression. We show that, though that an appropriate regularization is profitable for any coding, the choice of a relevant coding can prevail over the one of the regularization term for revealing structures. We focus on predictors though the issues raised also apply to responses. We illustrate our point on a classic data set. }
}

@incollection{hornik_2007,
  title={Deriving consensus rankings from benchmarking experiments},
  author={Hornik, Kurt and Meyer, David},
  booktitle={Advances in Data Analysis},
  pages={163--170},
  year={2007},
  publisher={Springer},
  doi={10.1007/978-3-540-70981-7_19}
}

@article{borda_1781,
  title={M{\'e}moire sur les {\'e}lections au scrutin},
  author={de Borda, Jean C},
  year={1781},
  publisher={Histoire de l$\backslash$'Academie Royale des Sciences}
}

@Book{kuhn_2019,
    author = {Kuhn, Max and Johnson, Kjell},
    title = {Feature Engineering and Selection: A Practical Approach for Predictive Models},
    year = {2019},
    url = {http://www.feat.engineering/index.html},
    publisher = {Chapman and Hall/CRC}
  }

@Book{wickham_2016,
    author = {Hadley Wickham},
    title = {ggplot2: Elegant Graphics for Data Analysis},
    publisher = {Springer-Verlag New York},
    year = {2016},
    isbn = {978-3-319-24277-4},
    url = {http://ggplot2.org}
  }

@article{probst_2019,
author = {Probst, Philipp and Wright, Marvin N. and Boulesteix, Anne-Laure},
title = {Hyperparameters and tuning strategies for random forest},
journal = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
year = {2019},
volume = {0},
number = {0},
pages = {e1301},
keywords = {ensemble, literature review, out-of-bag, performance evaluation, ranger, sequential model-based optimization, tuning parameter},
doi = {10.1002/widm.1301},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.1301},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1301},
abstract = {The random forest (RF) algorithm has several hyperparameters that have to be set by the user, for example, the number of observations drawn randomly for each tree and whether they are drawn with or without replacement, the number of variables drawn randomly for each split, the splitting rule, the minimum number of samples that a node must contain, and the number of trees. In this paper, we first provide a literature review on the parameters' influence on the prediction performance and on variable importance measures. It is well known that in most cases RF works reasonably well with the default values of the hyperparameters specified in software packages. Nevertheless, tuning the hyperparameters can improve the performance of RF. In the second part of this paper, after a presenting brief overview of tuning strategies, we demonstrate the application of one of the most established tuning strategies, model-based optimization (MBO). To make it easier to use, we provide the tuneRanger R package that tunes RF with MBO automatically. In a benchmark study on several datasets, we compare the prediction performance and runtime of tuneRanger with other tuning implementations in R and RF with default hyperparameters. This article is categorized under: Algorithmic Development > Biological Data Mining Algorithmic Development > Statistics Algorithmic Development > Hierarchies and Trees Technologies > Machine Learning}
}


@article{pedregosa_2011,
  title={Scikit-learn: Machine learning in Python},
  author={Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and others},
  journal={Journal of machine learning research},
  volume={12},
  number={Oct},
  pages={2825--2830},
  year={2011},
  url={http://www.jmlr.org/papers/volume12/pedregosa11a/pedregosa11a.pdf}
}

@inproceedings{weinberger_2009,
  title={Feature hashing for large scale multitask learning},
  author={Kilian Q. Weinberger and Anirban Dasgupta and John Langford and Alexander J. Smola and Josh Attenberg},
  booktitle={ICML},
  year={2009}
}

@Manual{wu_2018,
    title = {FeatureHashing: Creates a Model Matrix via Feature Hashing with a Formula
Interface},
    author = {Wush Wu and Michael Benesty},
    year = {2018},
    note = {R package version 0.9.1.3},
    url = {https://CRAN.R-project.org/package=FeatureHashing},
  }

@mastersthesis{coors_2018,
  title={Automatic Gradient Boosting},
  author={Coors, Stefan},
  year={2018},
  school={LMU Munich},
  url={https://epub.ub.uni-muenchen.de/59108/1/MA_Coors.pdf}
}

@article{hand_1997,
author = {Hand, D. J. and Henley, W. E.},
title = {Statistical Classification Methods in Consumer Credit Scoring: a Review},
journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
volume = {160},
number = {3},
pages = {523-541},
keywords = {Classification, Consumer loans, Credit control, Credit scoring, Discriminant analysis, Finance, Reject inference, Risk assessment},
doi = {10.1111/j.1467-985X.1997.00078.x},
url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-985X.1997.00078.x},
eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-985X.1997.00078.x},
abstract = {Credit scoring is the term used to describe formal statistical methods used for classifying applicants for credit into ‘good’ and ‘bad’ risk classes. Such methods have become increasingly important with the dramatic growth in consumer credit in recent years. A wide range of statistical methods has been applied, though the literature available to the public is limited for reasons of commercial confidentiality. Particular problems arising in the credit scoring context are examined and the statistical methods which have been applied are reviewed.},
year = {1997}
}

@article{fisher_1958,
author = { Walter D.   Fisher },
title = {On Grouping for Maximum Homogeneity},
journal = {Journal of the American Statistical Association},
volume = {53},
number = {284},
pages = {789-798},
year  = {1958},
publisher = {Taylor & Francis},
doi = {10.1080/01621459.1958.10501479},

URL = {
        https://www.tandfonline.com/doi/abs/10.1080/01621459.1958.10501479

},
eprint = {
        https://www.tandfonline.com/doi/pdf/10.1080/01621459.1958.10501479

}

}

@incollection{guyon_2017,
title = {LightGBM: A Highly Efficient Gradient Boosting Decision Tree},
author = {Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-Yan},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {3146--3154},
year = {2017},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf}
}


@Manual{ishwaran_2019,
    title = {Random Forests for Survival, Regression, and Classification
(RF-SRC)},
    author = {H. Ishwaran and U.B. Kogalur},
    publisher = {manual},
    year = {2019},
    note = {R package version 2.8.0},
    url = {https://cran.r-project.org/package=randomForestSRC},
    pdf = {https://cran.r-project.org/web/packages/randomForestSRC/randomForestSRC.pdf},
  }

@Article{liaw_2002,
    title = {Classification and Regression by randomForest},
    author = {Andy Liaw and Matthew Wiener},
    journal = {R News},
    year = {2002},
    volume = {2},
    number = {3},
    pages = {18-22},
    url = {https://CRAN.R-project.org/doc/Rnews/},
  }

@Manual{therneau_2018,
    title = {rpart: Recursive Partitioning and Regression Trees},
    author = {Terry Therneau and Beth Atkinson},
    year = {2018},
    note = {R package version 4.1-13},
    url = {https://CRAN.R-project.org/package=rpart},
  }

@article{coppersmith_1999,
  title={Partitioning nominal attributes in decision trees},
  author={Coppersmith, Don and Hong, Se June and Hosking, Jonathan RM},
  journal={Data Mining and Knowledge Discovery},
  volume={3},
  number={2},
  pages={197--217},
  year={1999},
  publisher={Springer},
  doi={10.1023/A:1009869804967}
}

@book{breiman_1984,
  title = {Classification and Regression Trees},
  publisher = {{CRC press}},
  author = {Breiman, Leo and Friedman, Jerome and Stone, Charles J. and Olshen, Richard A.},
  date = {1984},
}

@Manual{thomas_2018,
    title = {Automatic Gradient Boosting},
    author = {Janek Thomas and Stefan Coors and Bernd Bischl},
    year = {2018},
    url = {http://arxiv.org/abs/1807.03873v2},
  }

@incollection{feurer_2015,
title = {Efficient and Robust Automated Machine Learning},
author = {Feurer, Matthias and Klein, Aaron and Eggensperger, Katharina and Springenberg, Jost and Blum, Manuel and Hutter, Frank},
booktitle = {Advances in Neural Information Processing Systems 28},
editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
pages = {2962--2970},
year = {2015},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/5872-efficient-and-robust-automated-machine-learning.pdf}
}


@inproceedings{thornton_2013,
 author = {Thornton, Chris and Hutter, Frank and Hoos, Holger H. and Leyton-Brown, Kevin},
 title = {Auto-WEKA: Combined Selection and Hyperparameter Optimization of Classification Algorithms},
 booktitle = {Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
 series = {KDD '13},
 year = {2013},
 isbn = {978-1-4503-2174-7},
 location = {Chicago, Illinois, USA},
 pages = {847--855},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/2487575.2487629},
 doi = {10.1145/2487575.2487629},
 acmid = {2487629},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {hyperparameter optimization, model selection, weka},
}

@Article{muellner_2013,
    title = {{fastcluster}: Fast Hierarchical, Agglomerative Clustering Routines for {R} and {Python}},
    author = {Daniel M\"ullner},
    journal = {Journal of Statistical Software},
    year = {2013},
    volume = {53},
    number = {9},
    pages = {1--18},
    url = {http://www.jstatsoft.org/v53/i09/},
  }

@Article{vanschoren_2013,
    title = {{OpenML}: Networked Science in Machine Learning},
    author = {Joaquin Vanschoren and Jan {N. van Rijn} and Bernd Bischl and Luis Torgo},
    journal = {SIGKDD Explorations},
    volume = {15},
    year = {2013},
    pages = {49--60},
    doi = {10.1145/2641190.2641198},
    url = {https://dx.doi.org/10.1145/2641190.2641198},
  }

@Article{casalicchio_2017,
    title = {{OpenML}: An {R} Package to Connect to the Machine Learning Platform {OpenML}},
    author = {Giuseppe Casalicchio and Jakob Bossek and Michel Lang and Dominik Kirchhoff and Pascal Kerschke and Benjamin Hofner and Heidi Seibold and Joaquin Vanschoren and Bernd Bischl},
    journal = {Computational Statistics},
    year = {2017},
    pages = {1--15},
    doi = {10.1007/s00180-017-0742-2},
    url = {http://dx.doi.org/10.1007/s00180-017-0742-2},
  }

@Manual{binder_2018,
    title = {mlrCPO: Composable Preprocessing Operators and Pipelines for Machine Learning},
    author = {Binder, Martin},
    year = {2018},
    note = {R package version 0.3.4-2},
    url = {https://github.com/mlr-org/mlrCPO},
  }

@article{guo_2016,
  title={Entity embeddings of categorical variables},
  author={Guo, Cheng and Berkhahn, Felix},
  journal={arXiv preprint arXiv:1604.06737},
  year={2016}
}

@incollection{prokhorenkova_2018,
title = {CatBoost: unbiased boosting with categorical features},
author = {Prokhorenkova, Liudmila and Gusev, Gleb and Vorobev, Aleksandr and Dorogush, Anna Veronika and Gulin, Andrey},
booktitle = {Advances in Neural Information Processing Systems 31},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {6638--6648},
year = {2018},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7898-catboost-unbiased-boosting-with-categorical-features.pdf}
}


@Manual{kuhn2018,
    title = {embed: Extra Recipes for Encoding Categorical Predictors},
    author = {Max Kuhn},
    year = {2018},
    note = {R package version 0.0.2},
    url = {https://CRAN.R-project.org/package=embed},
  }

@Manual{mount2019,
    title = {vtreat: A Statistically Sound 'data.frame' Processor/Conditioner},
    author = {John Mount and Nina Zumel},
    year = {2019},
    note = {R package version 1.3.7},
    url = {https://CRAN.R-project.org/package=vtreat},
  }

@article{micci_barreca_2001,
 author = {Micci-Barreca, Daniele},
 title = {A Preprocessing Scheme for High-cardinality Categorical Attributes in Classification and Prediction Problems},
 journal = {SIGKDD Explor. Newsl.},
 issue_date = {July 2001},
 volume = {3},
 number = {1},
 month = jul,
 year = {2001},
 issn = {1931-0145},
 pages = {27--32},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/507533.507538},
 doi = {10.1145/507533.507538},
 acmid = {507538},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {categorical attributes, empirical bayes, hierarchical attributes, neural networks, predictive models},
}

@Article{steinwart_2017,
    title = {{liquidSVM}: A Fast and Versatile SVM package},
    author = {Ingo Steinwart and Philipp Thomann},
    year = {2017},
    journal = {{ArXiv e-prints 1702.06899}},
    primaryclass = {stat.ML},
    month = {feb},
    url = {http://www.isa.uni-stuttgart.de/software},
  }

@Manual{schliep_2016,
    title = {kknn: Weighted k-Nearest Neighbors},
    author = {Klaus Schliep and Klaus Hechenbichler},
    year = {2016},
    note = {R package version 1.3.1},
    url = {https://CRAN.R-project.org/package=kknn},
  }

@Manual{chen_2018,
    title = {xgboost: Extreme Gradient Boosting},
    author = {Chen, Tianqi and He, Tong and Benesty, Michael and Khotilovich, Vadim and Tang, Yuan and Cho, Hyunsu and Chen, Kailong and Mitchell, Rory and Cano, Ignacio and Zhou, Tianyi and Li, Mu and Xie, Junyuan and Lin, Min and Geng, Yifeng and Li, Yutian},
    year = {2018},
    note = {R package version 0.71.2},
    url = {https://CRAN.R-project.org/package=xgboost},
  }

@Article{friedman_2010,
    title = {Regularization Paths for Generalized Linear Models via
      Coordinate Descent},
    author = {Jerome Friedman and Trevor Hastie and Robert Tibshirani},
    journal = {Journal of Statistical Software},
    year = {2010},
    volume = {33},
    number = {1},
    pages = {1--22},
    url = {http://www.jstatsoft.org/v33/i01/},
    doi = {10.18637/jss.v033.i01}
  }

@Article{strobl_2007,
author="Strobl, Carolin
and Boulesteix, Anne-Laure
and Zeileis, Achim
and Hothorn, Torsten",
title="Bias in random forest variable importance measures: Illustrations, sources and a solution",
journal="BMC Bioinformatics",
year="2007",
month="Jan",
day="25",
volume="8",
number="1",
pages="25",
abstract="Variable importance measures for random forests have been receiving increased attention as a means of variable selection in many classification tasks in bioinformatics and related scientific fields, for instance to select a subset of genetic markers relevant for the prediction of a certain disease. We show that random forest variable importance measures are a sensible means for variable selection in many applications, but are not reliable in situations where potential predictor variables vary in their scale of measurement or their number of categories. This is particularly important in genomics and computational biology, where predictors often include variables of different types, for example when predictors include both sequence data and continuous variables such as folding energy, or when amino acid sequence data show different numbers of categories.",
issn="1471-2105",
doi="10.1186/1471-2105-8-25",
url="https://doi.org/10.1186/1471-2105-8-25"
}

@Article{strobl_2008,
author="Strobl, Carolin
and Boulesteix, Anne-Laure
and Kneib, Thomas
and Augustin, Thomas
and Zeileis, Achim",
title="Conditional variable importance for random forests",
journal="BMC Bioinformatics",
year="2008",
month="Jul",
day="11",
volume="9",
number="1",
pages="307",
abstract="Random forests are becoming increasingly popular in many scientific fields because they can cope with ``small n large p'' problems, complex interactions and even highly correlated predictor variables. Their variable importance measures have recently been suggested as screening tools for, e.g., gene expression studies. However, these variable importance measures show a bias towards correlated predictor variables.",
issn="1471-2105",
doi="10.1186/1471-2105-9-307",
url="https://doi.org/10.1186/1471-2105-9-307"
}

@article{ferri_2009,
title = "An experimental comparison of performance measures for classification",
journal = "Pattern Recognition Letters",
volume = "30",
number = "1",
pages = "27 - 38",
year = "2009",
issn = "0167-8655",
doi = "10.1016/j.patrec.2008.08.010",
url = "http://www.sciencedirect.com/science/article/pii/S0167865508002687",
author = "C. Ferri and J. Hernández-Orallo and R. Modroiu",
keywords = "Classification, Performance measures, Ranking, Calibration"
}

@book{kuhn_2013,
  title = {Applied Predictive Modeling},
  volume = {26},
  timestamp = {2017-08-23T08:50:03Z},
  publisher = {{Springer}},
  author = {Kuhn, Max and Johnson, Kjell},
  date = {2013}
}

@article{tibshirani_1996,
 ISSN = {00359246},
 URL = {http://www.jstor.org/stable/2346178},
 abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
 author = {Robert Tibshirani},
 journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
 number = {1},
 pages = {267-288},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Regression Shrinkage and Selection via the Lasso},
 volume = {58},
 year = {1996}
}

@article {zou_2005,
author = {Zou, Hui and Hastie, Trevor},
title = {Regularization and variable selection via the elastic net},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
volume = {67},
number = {2},
publisher = {Blackwell Publishing Ltd},
issn = {1467-9868},
url = {http://dx.doi.org/10.1111/j.1467-9868.2005.00503.x},
doi = {10.1111/j.1467-9868.2005.00503.x},
pages = {301--320},
keywords = {Grouping effect, LARS algorithm, Lasso, Penalization, p≫n problem, Variable selection},
year = {2005},
}

@Article{breiman_2001,
author="Breiman, Leo",
title="Random Forests",
journal="Machine Learning",
year="2001",
month="Oct",
day="01",
volume="45",
number="1",
pages="5--32",
abstract="Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund {\&} R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148--156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.",
issn="1573-0565",
doi="10.1023/A:1010933404324",
url="https://doi.org/10.1023/A:1010933404324"
}

@inproceedings{chen_2016,
 author = {Chen, Tianqi and Guestrin, Carlos},
 title = {XGBoost: A Scalable Tree Boosting System},
 booktitle = {Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
 series = {KDD '16},
 year = {2016},
 isbn = {978-1-4503-4232-2},
 location = {San Francisco, California, USA},
 pages = {785--794},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/2939672.2939785},
 doi = {10.1145/2939672.2939785},
 acmid = {2939785},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {large-scale machine learning},
}

@article{bischl_2012,
  title = {Resampling Methods for Meta-Model Validation with Recommendations for Evolutionary Computation},
  volume = {20},
  url = {http://www.mitpressjournals.org/doi/abs/10.1162/EVCO_a_00069},
  doi = {10.1162/EVCO_a_00069},
  number = {2},
  journaltitle = {Evolutionary Computation},
  author = {Bischl, Bernd and Mersmann, Olaf and Trautmann, Heike and Weihs, Claus},
  date = {2012},
  pages = {249--275}
}

@article{friedman_2001,
 ISSN = {00905364},
 URL = {http://www.jstor.org/stable/2699986},
 abstract = {Function estimation/approximation is viewed from the perspective of numerical optimization in function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest-descent minimization. A general gradient descent "boosting" paradigm is developed for additive expansions based on any fitting criterion. Specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification. Special enhancements are derived for the particular case where the individual additive components are regression trees, and tools for interpreting such "TreeBoost" models are presented. Gradient boosting of regression trees produces competitive, highly robust, interpretable procedures for both regression and classification, especially appropriate for mining less than clean data. Connections between this approach and the boosting methods of Freund and Shapire and Friedman, Hastie and Tibshirani are discussed.},
 author = {Jerome Friedman},
 journal = {The Annals of Statistics},
 number = {5},
 pages = {1189-1232},
 publisher = {Institute of Mathematical Statistics},
 title = {Greedy Function Approximation: A Gradient Boosting Machine},
 volume = {29},
 year = {2001}
}

@book{hastie_2009,
  title={The elements of statistical learning},
  author={Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  year={2009},
  publisher={Springer}
}

@Article{bischl_2016,
  title = {{mlr}: Machine Learning in R},
  author = {Bernd Bischl and Michel Lang and Lars Kotthoff and Julia Schiffner and Jakob Richter and Erich Studerus and Giuseppe Casalicchio and Zachary M. Jones},
  journal = {Journal of Machine Learning Research},
  year = {2016},
  volume = {17},
  number = {170},
  pages = {1-5},
  url = {http://jmlr.org/papers/v17/15-066.html},
}

@Manual{aust_2018,
  author = {Frederik Aust and Marius Barth},
  title = {{papaja}: {Create} {APA} manuscripts with {R Markdown}},
  year = {2018},
  note = {R package version 0.1.0.9842},
  url = {https://github.com/crsh/papaja},
}

@Article{lang_2017,
  title = {batchtools: Tools for R to work on batch systems},
  author = {Michel Lang and Bernd Bischl and Dirk Surmann},
  journal = {The Journal of Open Source Software},
  year = {2017},
  month = {feb},
  volume = {2},
  number = {10},
  doi = {10.21105/joss.00135},
  url = {https://doi.org/10.21105/joss.00135},
}

@Book{xie_2015,
  title = {Dynamic Documents with {R} and knitr},
  author = {Yihui Xie},
  publisher = {Chapman and Hall/CRC},
  address = {Boca Raton, Florida},
  year = {2015},
  edition = {2nd},
  note = {ISBN 978-1498716963},
  url = {https://yihui.name/knitr/},
}

@article{wright_2017,
    title = {{ranger}: A Fast Implementation of Random Forests for High Dimensional Data in {C++} and {R}},
    author = {Wright, Marvin N. and Ziegler, Andreas},
    journal = {Journal of Statistical Software},
    year = {2017},
    volume = {77},
    number = {1},
    pages = {1--17},
    doi = {10.18637/jss.v077.i01},
  }

@Article{bates_2015,
    title = {Fitting Linear Mixed-Effects Models Using {lme4}},
    author = {Douglas Bates and Martin M{\"a}chler and Ben Bolker and Steve Walker},
    journal = {Journal of Statistical Software},
    year = {2015},
    volume = {67},
    number = {1},
    pages = {1--48},
    doi = {10.18637/jss.v067.i01},
  }

@article{bates_2018,
  title={Computational methods for mixed models},
  author={Bates, Douglas},
  journal={Vignette for lme4},
  url = {https://cran.r-project.org/web/packages/lme4/vignettes/Theory.pdf},
  year={2020}
}

@book{gelman_2006,
  title={Data analysis using regression and multilevel/hierarchical models},
  author={Gelman, Andrew and Hill, Jennifer},
  year={2006},
  publisher={Cambridge university press}
}

@article{au_2018,
 author = {Au, Timothy C.},
 title = {Random Forests, Decision Trees, and Categorical Predictors: The "Absent Levels" Problem},
 journal = {J. Mach. Learn. Res.},
 issue_date = {January 2018},
 volume = {19},
 number = {1},
 month = jan,
 year = {2018},
 issn = {1532-4435},
 pages = {1737--1766},
 numpages = {30},
 url = {http://dl.acm.org/citation.cfm?id=3291125.3309607},
 acmid = {3309607},
 publisher = {JMLR.org},
 keywords = {CART, absent levels, categorical predictors, decision trees, random forests},
}

@article{wright_2019,
  title={Splitting on categorical predictors in random forests},
  author={Wright, Marvin N. and König, Inke R.},
  journal={PeerJ},
  volume={7},
  year={2019},
  publisher={PeerJ, Inc},
  doi={10.7717/peerj.6339}
}

@Article{ishwaran_2015,
author="Ishwaran, Hemant",
title="The effect of splitting on random forests",
journal="Machine Learning",
year="2015",
month="Apr",
day="01",
volume="99",
number="1",
pages="75--118",
abstract="The effect of a splitting rule on random forests (RF) is systematically studied for regression and classification problems. A class of weighted splitting rules, which includes as special cases CART weighted variance splitting and Gini index splitting, are studied in detail and shown to possess a unique adaptive property to signal and noise. We show for noisy variables that weighted splitting favors end-cut splits. While end-cut splits have traditionally been viewed as undesirable for single trees, we argue for deeply grown trees (a trademark of RF) end-cut splitting is useful because: (a) it maximizes the sample size making it possible for a tree to recover from a bad split, and (b) if a branch repeatedly splits on noise, the tree minimal node size will be reached which promotes termination of the bad branch. For strong variables, weighted variance splitting is shown to possess the desirable property of splitting at points of curvature of the underlying target function. This adaptivity to both noise and signal does not hold for unweighted and heavy weighted splitting rules. These latter rules are either too greedy, making them poor at recognizing noisy scenarios, or they are overly ECP aggressive, making them poor at recognizing signal. These results also shed light on pure random splitting and show that such rules are the least effective. On the other hand, because randomized rules are desirable because of their computational efficiency, we introduce a hybrid method employing random split-point selection which retains the adaptive property of weighted splitting rules while remaining computational efficient.",
issn="1573-0565",
doi="10.1007/s10994-014-5451-2",
url="https://doi.org/10.1007/s10994-014-5451-2"
}

@article{coussement_2017,
title = "A comparative analysis of data preparation algorithms for customer churn prediction: A case study in the telecommunication industry",
journal = "Decision Support Systems",
volume = "95",
pages = "27 - 36",
year = "2017",
issn = "0167-9236",
doi = "https://doi.org/10.1016/j.dss.2016.11.007",
url = "http://www.sciencedirect.com/science/article/pii/S0167923616302020",
author = "Kristof Coussement and Stefan Lessmann and Geert Verstraeten",
keywords = "Predictive analytics, Data preparation techniques, Churn prediction"
}

@article{moeyersoms_2015,
title = "Including high-cardinality attributes in predictive models: A case study in churn prediction in the energy sector",
journal = "Decision Support Systems",
volume = "72",
pages = "72 - 81",
year = "2015",
issn = "0167-9236",
doi = "https://doi.org/10.1016/j.dss.2015.02.007",
url = "http://www.sciencedirect.com/science/article/pii/S0167923615000275",
author = "Julie Moeyersoms and David Martens",
keywords = "Data mining, Predictive modeling, High-cardinality attributes, Churn prediction"
}

@InProceedings{grabczewski_2004,
author="Gr{\k{a}}bczewski, Krzysztof",
editor="Rutkowski, Leszek
and Siekmann, J{\"o}rg H.
and Tadeusiewicz, Ryszard
and Zadeh, Lotfi A.",
title="SSV Criterion Based Discretization for Naive Bayes Classifiers",
booktitle="Artificial Intelligence and Soft Computing - ICAISC 2004",
year="2004",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="574--579",
abstract="Decision tree algorithms deal with continuous variables by finding split points which provide best separation of objects belonging to different classes. Such criteria can also be used to augment methods which require or prefer symbolic data. A tool for continuous data discretization based on the SSV criterion (designed for decision trees) has been constructed. It significantly improves the performance of Naive Bayes Classifier. The combination of the two methods has been tested on 15 datasets from UCI repository and compared with similar approaches. The comparison confirms the robustness of the system.",
isbn="978-3-540-24844-6"
}

@inbook{boriah_2008,
author = {Shyam Boriah and Varun Chandola and Vipin Kumar},
title = {Similarity Measures for Categorical Data: A Comparative Evaluation},
booktitle = {Proceedings of the 2008 SIAM International Conference on Data Mining},
chapter = {},
pages = {243-254},
doi = {10.1137/1.9781611972788.22},
URL = {https://epubs.siam.org/doi/abs/10.1137/1.9781611972788.22},
eprint = {https://epubs.siam.org/doi/pdf/10.1137/1.9781611972788.22}
}

@Article{zhao_2018,
author="Zhao, Wentao
and Li, Qian
and Zhu, Chengzhang
and Song, Jianglong
and Liu, Xinwang
and Yin, Jianping",
title="Model-aware categorical data embedding: a data-driven approach",
journal="Soft Computing",
year="2018",
month="Jun",
day="01",
volume="22",
number="11",
pages="3603--3619",
abstract="Learning from categorical data is a critical yet challenging task. Current research focuses on either leveraging the complex interaction between and within categorical values to generate a numerical representation, or designing a model that can tackle this types of data directly. However, both of these paradigms overlook the relation between the data characteristics and learning model hypothesis. In this paper, we propose a model-aware categorical data embedding framework that jointly reveals the intrinsic categorical data characteristics and optimizes the fitness of the representation for the follow-up learning model. An ELM-aware and a SVM-aware representation methods have been instantiated under this framework. Extensive experiments of classification with the embedded representation on 17 data sets demonstrate that the proposed framework can significantly improve the categorical data representation performance compared with state-of-the-art competitors.",
issn="1433-7479",
doi="10.1007/s00500-018-3170-5",
url="https://doi.org/10.1007/s00500-018-3170-5"
}


@InProceedings{grabczewski_2003,
author="Gr{\k{a}}bczewski, Krzysztof
and Jankowski, Norbert",
editor="Kaynak, Okyay
and Alpaydin, Ethem
and Oja, Erkki
and Xu, Lei",
title="Transformations of Symbolic Data for Continuous Data Oriented Models",
booktitle="Artificial Neural Networks and Neural Information Processing --- ICANN/ICONIP 2003",
year="2003",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="359--366",
abstract="Most of Computational Intelligence models (e.g. neural networks or distance based methods) are designed to operate on continuous data and provide no tools to adapt their parameters to data described by symbolic values. Two new conversion methods which replace symbolic by continuous attributes are presented and compared to two commonly known ones. The advantages of the continuousification are illustrated with the results obtained with a neural network, SVM and a kNN systems for the converted data.",
isbn="978-3-540-44989-8"
}

@article{decock_2011,
author = {Dean De Cock},
title = {Ames, Iowa: Alternative to the Boston Housing Data as an End of Semester Regression Project},
journal = {Journal of Statistics Education},
volume = {19},
number = {3},
pages = {null},
year  = {2011},
publisher = {Taylor & Francis},
doi = {10.1080/10691898.2011.11889627},

URL = {
        https://doi.org/10.1080/10691898.2011.11889627

},
eprint = {
        https://doi.org/10.1080/10691898.2011.11889627

}

}

@article{albert_2015,
author = {Albert Y. Kim and Adriana Escobedo-Land},
title = {OkCupid Data for Introductory Statistics and Data Science Courses},
journal = {Journal of Statistics Education},
volume = {23},
number = {2},
pages = {null},
year  = {2015},
publisher = {Taylor & Francis},
doi = {10.1080/10691898.2015.11889737},

URL = {
        https://doi.org/10.1080/10691898.2015.11889737

},
eprint = {
        https://doi.org/10.1080/10691898.2015.11889737

}

}


@Article{cerda_2018,
author="Cerda, Patricio
and Varoquaux, Ga{\"e}l
and K{\'e}gl, Bal{\'a}zs",
title="Similarity encoding for learning with dirty categorical variables",
journal="Machine Learning",
year="2018",
month="Sep",
day="01",
volume="107",
number="8",
pages="1477--1494",
abstract="For statistical learning, categorical variables in a table are usually considered as discrete entities and encoded separately to feature vectors, e.g., with one-hot encoding. ``Dirty'' non-curated data give rise to categorical variables with a very high cardinality but redundancy: several categories reflect the same entity. In databases, this issue is typically solved with a deduplication step. We show that a simple approach that exposes the redundancy to the learning algorithm brings significant gains. We study a generalization of one-hot encoding, similarity encoding, that builds feature vectors from similarities across categories. We perform a thorough empirical validation on non-curated tables, a problem seldom studied in machine learning. Results on seven real-world datasets show that similarity encoding brings significant gains in predictive performance in comparison with known encoding methods for categories or strings, notably one-hot encoding and bag of character n-grams. We draw practical recommendations for encoding dirty categories: 3-gram similarity appears to be a good choice to capture morphological resemblance. For very high-cardinalities, dimensionality reduction significantly reduces the computational cost with little loss in performance: random projections or choosing a subset of prototype categories still outperform classic encoding approaches.",
issn="1573-0565",
doi="10.1007/s10994-018-5724-2",
url="https://doi.org/10.1007/s10994-018-5724-2"
}

@ARTICLE{cerda_2020,
  author={P. {Cerda} and G. {Varoquaux}},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  title={Encoding high-cardinality string categorical variables},
  year={2020},
  volume={},
  number={},
  pages={1-1},
  doi={10.1109/TKDE.2020.2992529}
}

@software{will_mcginnis_2018_1157110,
  author       = {Will McGinnis and
                  hbghhy and
                  Wenwu Tao and
                  andrethrill and
                  Chapman Siu and
                  Cameron Davison and
                  Nicholas Bollweg},
  title        = {{scikit-learn-contrib/categorical-encoding: Release
                   for zenodo}},
  month        = jan,
  year         = 2018,
  publisher    = {Zenodo},
  version      = {1.2.6},
  doi          = {10.5281/zenodo.1157110},
  url          = {https://doi.org/10.5281/zenodo.1157110}
}

@misc{prokopev_2018, title={Mean (likelihood) encodings: a comprehensive study}, url={https://www.kaggle.com/vprokopev/mean-likelihood-encodings-a-comprehensive-study}, journal={Kaggle Forums}, publisher={Kaggle}, author={Prokopev, Viacheslav}, year={2018}, month={Oct}}

@article{brown2012,
title = "Conditional likelihood maximisation: A unifying framework for information theoretic feature selection",
abstract = "We present a unifying framework for information theoretic feature selection, bringing almost two decades of research on heuristic filter criteria under a single theoretical interpretation. This is in response to the question: {"}what are the implicit statistical assumptions of feature selection criteria based on mutual information?{"}. To answer this, we adopt a different strategy than is usual in the feature selection literature-instead of trying to define a criterion, we derive one, directly from a clearly specified objective function: the conditional likelihood of the training labels. While many hand-designed heuristic criteria try to optimize a definition of feature 'relevancy' and 'redundancy', our approach leads to a probabilistic framework which naturally incorporates these concepts. As a result we can unify the numerous criteria published over the last two decades, and show them to be low-order approximations to the exact (but intractable) optimisation problem. The primary contribution is to show that common heuristics for information based feature selection (including Markov Blanket algorithms as a special case) are approximate iterative maximisers of the conditional likelihood. A large empirical study provides strong evidence to favour certain classes of criteria, in particular those that balance the relative size of the relevancy/redundancy terms. Overall we conclude that the JMI criterion (Yang and Moody, 1999; Meyer et al., 2008) provides the best tradeoff in terms of accuracy, stability, and flexibility with small data samples. {\textcopyright} 2012 Gavin Brown, Adam Pocock, Ming-jie Zhao and Mikel Luj{\'a}n.",
keywords = "Conditional likelihood, Feature selection, Mutual information",
author = "Gavin Brown and Adam Pocock and Zhao Ming-Jie and Mikel Luj{\'a}n",
year = "2012",
month = jan,
day = "8",
language = "English",
volume = "13",
pages = "27--66",
journal = "Journal of Machine Learning Research",
issn = "1533-7928",
publisher = "Microtome Publishing",
}

  @Manual{mlr3pipelines,
    title = {mlr3pipelines: Preprocessing Operators and Pipelines for 'mlr3'},
    author = {Martin Binder and Florian Pfisterer and Lennart Schneider and Bernd Bischl and Michel Lang and Susanne Dandl},
    year = {2020},
    note = {R package version 0.3.1},
    url = {https://CRAN.R-project.org/package=mlr3pipelines},
  }

@misc{dehghani2021benchmark,
      title={The Benchmark Lottery},
      author={Mostafa Dehghani and Yi Tay and Alexey A. Gritsenko and Zhe Zhao and Neil Houlsby and Fernando Diaz and Donald Metzler and Oriol Vinyals},
      year={2021},
      eprint={2107.07002},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{niesl2021overoptimism,
      title={Over-optimism in benchmark studies and the multiplicity of design and analysis options when interpreting their results},
      author={Christina Nießl and Moritz Herrmann and Chiara Wiedemann and Giuseppe Casalicchio and Anne-Laure Boulesteix},
      year={2021},
      eprint={2106.02447},
      archivePrefix={arXiv},
      primaryClass={stat.ME}
}